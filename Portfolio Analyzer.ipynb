{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1bc06e-b720-4bb6-8c8d-eb0db266d11e",
   "metadata": {},
   "source": [
    "# Portfolio Analyzer\n",
    "\n",
    "**Author:** Yusuf Ardahan Dogru  \n",
    "**Date:** December 2025  \n",
    "**Purpose:** Analyze multi-asset portfolios using Modern Portfolio Theory\n",
    "\n",
    "## Overview\n",
    "This notebook explores implementations and outcomes of portfolio optimization techniques that the author is curious about; including:\n",
    "- Risk-return analysis\n",
    "- Minimum variance optimization\n",
    "- Sharpe ratio maximization\n",
    "\n",
    "The notebook also contains the XGBoost model to forecast stock values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff6dd0-2d04-42ac-b6d4-1431c04b2588",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94f089-09e4-41e3-a7eb-29139977b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74ab93f-678c-4f9a-94f4-0f013fd671fc",
   "metadata": {},
   "source": [
    "### Style of Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663792e-cf03-4e03-a129-93fea037e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4beb4-4ba9-40b2-9a79-121447824374",
   "metadata": {},
   "source": [
    "### Stock and ETF tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f482f1-20b6-4e70-9006-0c47149ae672",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    # Individual stocks (play around with different stocks)\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'NVDA', # 4 of the Magnificient 7\n",
    "    'JPM', 'BAC',                    # finance/banking giants\n",
    "    'WMT', 'AMZN',                   # retail giants ( + 1 Magnif 7)\n",
    "    'JNJ', 'UNH',                    # healthcare\n",
    "    'XOM', 'CVX',                    # oil\n",
    "    \n",
    "    # Strategic ETFs\n",
    "    'SCHG',  # Growth\n",
    "    'SPMO',  # Momentum\n",
    "    'SMH',   # Semiconductors\n",
    "    \n",
    "    # Benchmarks\n",
    "    'SPY',   # S&P 500\n",
    "    'AGG'    # Bonds (consider for diversification)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd249bd-b129-490c-9d56-60cb07bc855b",
   "metadata": {},
   "source": [
    "### Get ticker data using yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d2449-2ff7-4371-9deb-6e9514ddded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2022-12-01'\n",
    "end_date = '2024-12-01'\n",
    "\n",
    "# auto_adjust=True => modifies historical stock prices so that stock splits and dividends do not artificially distort returns\n",
    "# progress=False => this just removes the progress bar, just a visual setting\n",
    "raw_data = yf.download(tickers, start=start_date, end=end_date, progress=False, auto_adjust=True)  \n",
    "\n",
    "\n",
    "if isinstance(raw_data.columns, pd.MultiIndex):\n",
    "    data = raw_data['Close'].copy()\n",
    "else:\n",
    "    data = raw_data.copy()\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Date range: From {data.index[0].date()} to {data.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1660a15-b71e-4654-963e-25a00792d6a1",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d203d5-4125-44a3-a5fa-d6d6eacb86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (data.isnull().sum()).sum() > 0:\n",
    "    print(f\"\\nMissing values per stock:\")\n",
    "    print(data.isnull().sum())\n",
    "    data = data.ffill()  \n",
    "# ffill => forward fill => replace with last/preceding val\n",
    "\n",
    "if len(data) < 100:\n",
    "    print(f\"Warning: Only {len(data)} days of data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3057586-41d6-4dbf-b6de-652c582d8e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = data.pct_change().dropna()\n",
    "#pct_change is a pandas Dataframe method, computes: (current_value - previous_value) / previous_value for each column, row by row\n",
    "\n",
    "print(\"Daily returns calculated!\")\n",
    "print(f\"Shape: {returns.shape}\")\n",
    "print(f\"\\nSample returns:\")\n",
    "print(returns.head(1))\n",
    "\n",
    "# Calculate key metrics\n",
    "annual_returns = returns.mean() * 252  # 252 trading days per year\n",
    "annual_volatility = returns.std() * np.sqrt(252)\n",
    "sharpe_ratio = annual_returns / annual_volatility\n",
    "\n",
    "# Create summary DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Annual Return (%)': (annual_returns * 100).round(2),\n",
    "    'Volatility (%)': (annual_volatility * 100).round(2),\n",
    "    'Sharpe Ratio': sharpe_ratio.round(3)\n",
    "}).sort_values('Sharpe Ratio', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PORTFOLIO METRICS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0145ac7-91dd-4db2-8433-4bbf249aac5c",
   "metadata": {},
   "source": [
    "### Check Correlations of Stock With Each Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d6c87-2501-4b52-a6b0-8ea839a40400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_matrix = returns.corr()\n",
    "\n",
    "plt.figure(figsize=(24, 16))  # Larger figure\n",
    "sns.heatmap(corr_matrix, \n",
    "            cmap='coolwarm', \n",
    "            center=0, \n",
    "            linewidths=0.5,\n",
    "            annot=True,  # Show correlation values\n",
    "            fmt='.2f',   # 2 decimal places\n",
    "            square=True, # Make cells square\n",
    "            cbar_kws={'shrink': 0.8})  # Smaller colorbar\n",
    "plt.title('Asset Return Correlation Heatmap', fontsize=16, pad=20)\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x labels\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()  # Prevent label cutoff\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712138da-5d36-4294-9154-6c2b4ed56173",
   "metadata": {},
   "source": [
    "## Construct a fair portfolio (equal allocation in the porfolio for each ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f92c1-0aed-4be3-9b21-29a41feb81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "riskFreeRate = 0.04 # 3 month T-bills as the risk-free benchmark.\n",
    "\n",
    "num_assets = returns.shape[1] #num of stock tickers\n",
    "equal_weights = np.array([1 / num_assets] * num_assets) #equal weights that add upto one\n",
    "\n",
    "portfolio_returns_fair = returns @ equal_weights # daily returns of fairly allocated portfolio ( 1 / numStocks)\n",
    "\n",
    "portfolio_vol_fair = portfolio_returns_fair.std() * np.sqrt(252)  # avg daily standard deviation * 252 for annualized volatility\n",
    "portfolio_return_fair = portfolio_returns_fair.mean() * 252 # avg daily return * 252 to annualize\n",
    "portfolio_sharpe_fair = portfolio_return_fair / portfolio_vol_fair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58588869-0350-4d41-9ad6-0e8099f237a5",
   "metadata": {},
   "source": [
    "## Optimize Fair Portfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf54ffe-733d-4471-9599-6193317f3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = returns.cov() * 252\n",
    "\n",
    "def portfolio_volatility(weights):\n",
    "    return np.sqrt(weights.T @ cov_matrix.values @ weights)\n",
    "\n",
    "# sum of weights must be equal to 1\n",
    "constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "\n",
    "#each weight is between 0 and 1; a percentage of the portfolio\n",
    "bounds = tuple((0, 1) for _ in range(num_assets))\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "initial_weights = equal_weights\n",
    "\n",
    "# minimize portfolio volatility using initial weights according to SLSQP method with bounds within 0 and 1 and the constraint above\n",
    "opt_result = minimize(\n",
    "    portfolio_volatility,\n",
    "    initial_weights,\n",
    "    method='SLSQP',\n",
    "    bounds=bounds,\n",
    "    constraints=constraints\n",
    ")\n",
    "\n",
    "opt_weights = opt_result.x\n",
    "print(opt_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8f567-294f-4619-97e9-6167e117d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_returns_opt = returns @ opt_weights\n",
    "\n",
    "portfolio_vol_opt = portfolio_returns_opt.std() * np.sqrt(252)\n",
    "portfolio_return_opt = portfolio_returns_opt.mean() * 252\n",
    "portfolio_sharpe_opt = portfolio_return_opt / portfolio_vol_opt\n",
    "\n",
    "print(portfolio_sharpe_opt)\n",
    "print(portfolio_return_opt)\n",
    "print(portfolio_vol_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46023253-ae91-431f-9d5a-42585f698439",
   "metadata": {},
   "source": [
    "### Risk - Return Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50abfe8-49ec-4415-b353-ac784b128b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sharpe_ratio = (annual_returns - riskFreeRate) / annual_volatility\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot individual assets\n",
    "plt.scatter(\n",
    "    annual_volatility * 100,\n",
    "    annual_returns * 100,\n",
    "    s=100,\n",
    "    alpha=0.7,\n",
    "    c=sharpe_ratio,  # Color by Sharpe ratio\n",
    "    cmap='RdYlGn',   # Red (low) to Green (high)\n",
    "    label='Individual Assets'\n",
    ")\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "\n",
    "# Annotate each ticker\n",
    "for ticker in tickers:\n",
    "    plt.annotate(\n",
    "        ticker,\n",
    "        xy=(annual_volatility[ticker] * 100, annual_returns[ticker] * 100),\n",
    "        xytext=(5, 5),  # Offset the label slightly\n",
    "        textcoords='offset points',\n",
    "        fontsize=9,\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "# Plot portfolios\n",
    "plt.scatter(\n",
    "    portfolio_vol_eq * 100,\n",
    "    portfolio_return_eq * 100,\n",
    "    marker='*',\n",
    "    s=500,\n",
    "    c='red',\n",
    "    edgecolors='black',\n",
    "    linewidths=2,\n",
    "    label='Equal-Weighted Portfolio',\n",
    "    zorder=5  # Draw on top\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    portfolio_vol_opt * 100,\n",
    "    portfolio_return_opt * 100,\n",
    "    marker='*',\n",
    "    s=500,\n",
    "    c='green',\n",
    "    edgecolors='black',\n",
    "    linewidths=2,\n",
    "    label='Optimized Portfolio',\n",
    "    zorder=5\n",
    ")\n",
    "\n",
    "plt.xlabel('Volatility (%)', fontsize=12)\n",
    "plt.ylabel('Annual Return (%)', fontsize=12)\n",
    "plt.title('Riskâ€“Return Profile: Assets vs Portfolios', fontsize=14, pad=20)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e20462a-9bb7-4dbd-9c41-336dd167ef88",
   "metadata": {},
   "source": [
    "##  1: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e4ca9f-59df-4c96-9222-9b029871d50e",
   "metadata": {},
   "source": [
    "### Create predictive features from historical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c31e045-cea1-441f-940c-4575984394f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data, returns, ticker):\n",
    "\n",
    "    df = pd.DataFrame(index=returns.index)\n",
    "    print(df.columns)\n",
    "    # Momentum \n",
    "    df['return_5d'] = returns[ticker].rolling(5).mean()\n",
    "    df['return_20d'] = returns[ticker].rolling(20).mean()\n",
    "    df['return_60d'] = returns[ticker].rolling(60).mean()\n",
    "    \n",
    "    # Volatility \n",
    "    df['volatility_20d'] = returns[ticker].rolling(20).std()\n",
    "    df['volatility_60d'] = returns[ticker].rolling(60).std()\n",
    "    \n",
    "    # Volume  (if available)\n",
    "    # df['volume_ratio'] = volume[ticker] / volume[ticker].rolling(20).mean()\n",
    "    \n",
    "    # Market correlation \n",
    "    df['corr_to_spy'] = returns[ticker].rolling(60).corr(returns['SPY'])\n",
    "    \n",
    "    # Relative strength\n",
    "    df['rsi_14'] = calculate_rsi(returns[ticker], periods=14)\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "# Answers the question: How strong are the gains compared to the losses over a period?\n",
    "# also a momentum indicator \n",
    "def calculate_rsi(returns, periods=14):\n",
    "    delta = returns\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window = periods).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window = periods).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56234ba7-1e2e-4de1-867a-df28680a5946",
   "metadata": {},
   "source": [
    "### Create target variable (in this case, SPY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e750d3c-2982-41ae-88ae-a7da66d429c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(returns, ticker, forward_window=20):\n",
    "    \"\"\"\n",
    "    Will this stock outperform SPY in next 20 days? => 1 if yes, 0 if no\n",
    "    \"\"\"\n",
    "    # forward returns\n",
    "    forward_return_stock = returns[ticker].shift(-forward_window).rolling(forward_window).sum()\n",
    "    forward_return_spy = returns['SPY'].shift(-forward_window).rolling(forward_window).sum()\n",
    "    \n",
    "    target = (forward_return_stock > forward_return_spy).astype(int)\n",
    "    \n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1adf98-892a-4e04-80b9-f1bbb56e2804",
   "metadata": {},
   "source": [
    "### Combine Features & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c69176-d2ed-4511-a24c-eae590fa236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each stock, create dataset\n",
    "all_data = []\n",
    "\n",
    "for ticker in [t for t in tickers if t not in ['SPY', 'QQQ']]:\n",
    "    features = create_features(data, returns, ticker)\n",
    "    target = create_target(returns, ticker, forward_window=20)\n",
    "    \n",
    "    # Combine\n",
    "    stock_data = features.copy()\n",
    "    stock_data['target'] = target\n",
    "    stock_data['ticker'] = ticker\n",
    "    stock_data.dropna(inplace=True)\n",
    "    \n",
    "    all_data.append(stock_data)\n",
    "\n",
    "# Combine all stocks\n",
    "ml_dataset = pd.concat(all_data, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b12b1-7cc8-4000-8a3d-ae7929588668",
   "metadata": {},
   "source": [
    "### Time-based Train-Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd4df0-2b15-4f2f-bd7f-fe0b1f690eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "end_date = pd.to_datetime(end_date)  # Your end_date variable\n",
    "test_days = 120  # Last 120 days for testing\n",
    "split_date = end_date - pd.Timedelta(days=test_days)\n",
    "\n",
    "train = ml_dataset[ml_dataset.index < split_date]\n",
    "test = ml_dataset[ml_dataset.index >= split_date]\n",
    "\n",
    "X_train = train.drop(['target', 'ticker'], axis=1)\n",
    "y_train = train['target']\n",
    "\n",
    "X_test = test.drop(['target', 'ticker'], axis=1)\n",
    "y_test = test['target']\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Positive class %: {y_train.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119fe14-003f-479a-b786-4b5a1926cb66",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a53b50-1d56-44e9-a768-08c8da2b6dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abbe8a7-bc86-4bb4-9286-bb6de20603e8",
   "metadata": {},
   "source": [
    "#### Competitor Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59ed8e-ea19-4056-a89f-5c70c588ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict_proba(X_test)[:, 1]\n",
    "lr_auc = roc_auc_score(y_test, lr_pred)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict_proba(X_test)[:, 1]\n",
    "rf_auc = roc_auc_score(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1559017-9906-4d99-9f7d-42b443e62e67",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa52032-3e8d-413f-a7c5-c0c6fb0726ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_pred = xgb.predict_proba(X_test)[:, 1]\n",
    "xgb_auc = roc_auc_score(y_test, xgb_pred)\n",
    "\n",
    "# Compare\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
    "    'ROC-AUC': [lr_auc, rf_auc, xgb_auc]\n",
    "}).sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf37f8-b445-4a44-8978-add0f8c72e09",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65febe-2d72-4c53-8d3d-e9cb25389b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 5 Features:\")\n",
    "print(feature_importance.head())\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, xgb.predict(X_test)))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_estimator(xgb, X_test, y_test)\n",
    "plt.title('XGBoost Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ff4ca-be52-4eef-bd5b-d260196b2692",
   "metadata": {},
   "source": [
    "### Use Predictions for Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44deef86-8bcd-4995-9490-d5ef8a63e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current predictions for each stock\n",
    "current_features = {}\n",
    "for ticker in [t for t in tickers if t not in ['SPY', 'QQQ']]:\n",
    "    features = create_features(data, returns, ticker)\n",
    "    current_features[ticker] = features.iloc[-1:] # Most recent\n",
    "\n",
    "# Predict outperformance probability\n",
    "predictions = {}\n",
    "for ticker, feats in current_features.items():\n",
    "    prob = xgb.predict_proba(feats)[:, 1][0]\n",
    "    predictions[ticker] = prob\n",
    "\n",
    "# Create ML-based weights\n",
    "ml_weights = pd.Series(predictions)\n",
    "ml_weights = ml_weights / ml_weights.sum()  # Normalize to sum to 1\n",
    "\n",
    "print(\"ML-Based Portfolio Weights:\")\n",
    "print(ml_weights.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda16d5-14c0-4e59-aabe-2028eed30681",
   "metadata": {},
   "source": [
    "## Backtest ML Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cc123-aa5a-42ec-9ff2-7df9c4f8ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ML portfolio returns\n",
    "ml_portfolio_returns = test_returns @ ml_weights\n",
    "\n",
    "# Compare to benchmarks\n",
    "comparison = pd.DataFrame({\n",
    "    'ML Portfolio': [\n",
    "        ml_portfolio_returns.mean() * 252,\n",
    "        ml_portfolio_returns.std() * np.sqrt(252),\n",
    "        (ml_portfolio_returns.mean() * 252) / (ml_portfolio_returns.std() * np.sqrt(252))\n",
    "    ],\n",
    "    'Equal Weight': [\n",
    "        portfolio_return_eq,\n",
    "        portfolio_vol_eq,\n",
    "        portfolio_sharpe_eq\n",
    "    ],\n",
    "    'Min Variance': [\n",
    "        portfolio_return_opt,\n",
    "        portfolio_vol_opt,\n",
    "        portfolio_sharpe_opt\n",
    "    ]\n",
    "}, index=['Annual Return', 'Volatility', 'Sharpe Ratio'])\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dbf94a-3604-4aec-9340-9989602a1b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Autocorrelation of PRICES (high)\n",
    "plot_acf(data['AAPL'].dropna(), lags=30)\n",
    "plt.title('Autocorrelation: AAPL Prices')\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation of RETURNS (low)\n",
    "plot_acf(returns['AAPL'].dropna(), lags=30)\n",
    "plt.title('Autocorrelation: AAPL Returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f93e5-cca7-42d4-a072-604c4f59b758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee29ea8-5e1a-4ed4-8425-a0005d6e3fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395296ce-aac8-4282-ab6d-2e928176cb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74d16b-b2b1-445d-a462-a32b2c43f679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d05d58-efb4-4ef2-a93c-5bbf5f0b1af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a5146f-88f5-459e-88d0-0dfe2caca22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d61f6-a688-4590-a6cd-646e682485db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78458257-a18f-4778-91b9-e2afa0e99d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b983c-71bb-4195-9934-2941b68b53f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e7e36-c09b-4fb7-a822-8d5c55d75985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e5f1b-69f5-4d3c-86d6-aaa7887f065f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6e75d-0498-4b23-8014-b714ce3fbc87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f07784-f5ee-4425-be1c-c38e4bb97af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
